{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tflearn\n",
    "from sklearn.utils import shuffle\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "tflearn.activations\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(net, data, label):\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=3, tensorboard_dir='./learn_logs/')\n",
    "    model.fit({'input': data[\"X_train\"]}, {'target': data[\"y_train\"]},\n",
    "              validation_set=({'input': data[\"X_test\"]}, {'target': data[\"y_test\"]}),\n",
    "              n_epoch=100,\n",
    "              snapshot_step=1000,\n",
    "              show_metric=True,\n",
    "              run_id=label,\n",
    "              batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def task_1(activaction_='tanh'):\n",
    "    network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "    network = fully_connected(network, n_units=1024 * 4, activation='tanh')\n",
    "    network = fully_connected(network, n_units=1024 * 2, activation=activaction_)\n",
    "    network = fully_connected(network, n_units=1024 * 1, activation=activaction_)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='sgd', learning_rate=0.001, loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def task_2(activaction_='tanh'):\n",
    "    network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "    network = conv_2d(network, nb_filter=3, filter_size=3, strides=1, activation='relu')\n",
    "    network = fully_connected(network, 1024 * 4, activation=activaction_)\n",
    "    network = fully_connected(network, 1024 * 2, activation=activaction_)\n",
    "    network = fully_connected(network, 1024 * 1, activation=activaction_)\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def task_3():\n",
    "    network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "\n",
    "    network = conv_2d(network, nb_filter=3, filter_size=3, strides=1, activation='relu')\n",
    "    network = max_pool_2d(network, kernel_size=3, strides=2)\n",
    "\n",
    "    network = conv_2d(network, nb_filter=3, filter_size=3, strides=1, activation='relu')\n",
    "    network = max_pool_2d(network, kernel_size=3, strides=2)\n",
    "\n",
    "    network = fully_connected(network, 1024 * 4, activation='relu')\n",
    "    network = fully_connected(network, 1024 * 2, activation='relu')\n",
    "    network = fully_connected(network, 1024 * 1, activation='relu')\n",
    "\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dop_task_1():\n",
    "    network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "\n",
    "    network = conv_2d(network, nb_filter=3, filter_size=3, strides=1, activation='relu')\n",
    "    network = max_pool_2d(network, kernel_size=3, strides=2)\n",
    "\n",
    "    network = conv_2d(network, nb_filter=3, filter_size=3, strides=1, activation='relu')\n",
    "    network = max_pool_2d(network, kernel_size=3, strides=2)\n",
    "\n",
    "    network = fully_connected(network, 1024 * 4, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 1024 * 2, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 1024 * 1, activation='relu')\n",
    "\n",
    "    network = fully_connected(network, 10, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.007, loss='categorical_crossentropy', name='target')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"notMNIST.pickle\", allow_pickle=True)\n",
    "\n",
    "X, Y = data['test_dataset'], data['test_labels']\n",
    "X_val, y_val = data['valid_dataset'][:1000], data['valid_labels'][:1000]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split data on train and test\n",
    "X, Y = shuffle(X, Y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.2, stratify=Y)\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1, 28, 28, 1])\n",
    "y_train = dense_to_one_hot(y_train, 10)\n",
    "\n",
    "X_test = X_test.reshape([-1, 28, 28, 1])\n",
    "y_test = dense_to_one_hot(y_test)\n",
    "\n",
    "X_val = X_val.reshape([-1, 28, 28, 1])\n",
    "y_val = dense_to_one_hot(y_val)\n",
    "\n",
    "data_dict = {\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test,\n",
    "    \"X_val\": X_val,\n",
    "    \"y_val\": y_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for activation in ['sigmoid', 'relu', 'tanh']:\n",
    "    fit_and_predict(task_1(activation), data_dict, \"_\".join([\"task_1\", activation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_and_predict(task_2(), data_dict, \"_\".join([\"task_2\", \"tanh\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_and_predict(task_3(), data_dict, \"_\".join([\"task_3\", \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_and_predict(dop_task_1(), data_dict, \"_\".join([\"dop_task\", \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_and_predict(task_3(), data_dict, \"_\".join([\"task_3_withoutCUDA\", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.ranking import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.Normalizer()\n",
    "df = pd.read_csv(\"spambase_new.csv\", header=0)\n",
    "\n",
    "Y = df[\"label\"]\n",
    "Y = np.array(Y)\n",
    "del df['label']\n",
    "\n",
    "X = df\n",
    "X = np.array(X)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "print 'score Scikit learn: ', clf.score(X_test, Y_test)\n",
    "\n",
    "def Hypothesis(theta, x):\n",
    "    def Sigmoid(z):\n",
    "        G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0 * z))))\n",
    "        return G_of_Z\n",
    "\n",
    "    z = 0\n",
    "    for i in xrange(len(theta)):\n",
    "        z += x[i] * theta[i]\n",
    "    return Sigmoid(z)\n",
    "\n",
    "\n",
    "\n",
    "def Gradient_Descent(X, Y, theta, m, alpha):\n",
    "    def Cost_Function_Derivative(X, Y, theta, j, m, alpha):\n",
    "        sumErrors = 0\n",
    "        for i in xrange(m):\n",
    "            xi = X[i]\n",
    "            xij = xi[j]\n",
    "            hi = Hypothesis(theta, X[i])\n",
    "            error = (hi - Y[i]) * xij\n",
    "            sumErrors += error\n",
    "        m = len(Y)\n",
    "        constant = float(alpha) / float(m)\n",
    "        J = constant * sumErrors\n",
    "        return J\n",
    "\n",
    "    new_theta = []\n",
    "    constant = alpha / m\n",
    "    for j in xrange(len(theta)):\n",
    "        CFDerivative = Cost_Function_Derivative(X, Y, theta, j, m, alpha)\n",
    "        new_theta_value = theta[j] - CFDerivative\n",
    "        new_theta.append(new_theta_value)\n",
    "    return new_theta\n",
    "\n",
    "\n",
    "\n",
    "def Logistic_Regression(X, Y, alpha, theta, num_iters):\n",
    "    def Cost_Function(X, Y, theta, m):\n",
    "        sumOfErrors = 0\n",
    "        for i in xrange(m):\n",
    "            xi = X[i]\n",
    "            hi = Hypothesis(theta, xi)\n",
    "            if Y[i] == 1:\n",
    "                error = Y[i] * math.log(hi)\n",
    "            elif Y[i] == 0:\n",
    "                error = (1 - Y[i]) * math.log(1 - hi)\n",
    "            sumOfErrors += error\n",
    "        const = -1 / m\n",
    "        J = const * sumOfErrors\n",
    "        print 'cost is ', J\n",
    "        return J\n",
    "\n",
    "    def Declare_Winner(theta):\n",
    "        score = 0\n",
    "        winner = \"\"\n",
    "        scikit_score = clf.score(X_test, Y_test)\n",
    "        length = len(X_test)\n",
    "        probas = []\n",
    "        for i in xrange(length):\n",
    "            proba = Hypothesis(X_test[i], theta)\n",
    "            probas.append(proba)\n",
    "            prediction = round(proba)\n",
    "            answer = Y_test[i]\n",
    "            if prediction == answer:\n",
    "                score += 1\n",
    "\n",
    "        my_score = float(score) / float(length)\n",
    "        print 'Your score: ', my_score\n",
    "        print 'Scikits score: ', scikit_score\n",
    "\n",
    "        print \"ROC_AUC_SCORE: \", roc_auc_score(y_true=Y_test, y_score=probas)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(Y_test, probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        import matplotlib.pylab as plt\n",
    "\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    m = len(Y)\n",
    "    for x in xrange(num_iters):\n",
    "        new_theta = Gradient_Descent(X, Y, theta, m, alpha)\n",
    "        theta = new_theta\n",
    "        if x % 50 == 0:\n",
    "            print x, Cost_Function(X, Y, theta, m)\n",
    "    Declare_Winner(theta)\n",
    "\n",
    "\n",
    "initial_theta = np.random.uniform(-0.01, 0.01, (len(df.columns),))\n",
    "alpha = 0.8\n",
    "iterations = 1000\n",
    "Logistic_Regression(X, Y, alpha, initial_theta, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
